<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Using Requests Session Objects for web scraping | Bruno P. Kinoshita</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Using Requests Session Objects for web scraping" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I had to write a Python script some months ago, that would retrieve Solar energy data from a web site. It was basically a handful of HTTP calls, parse the response that was mainly in JSON, and store the results as JSON and CSV for processing later. Since it was such a small task, I used the Requests module instead of a complete web scraper. The HTTP requests had to be made with a time out, and also pass certain headers. I started customizing each call, until I learned about the Requests Session Objects. You create a session, as in ORM/JPA, where you can define a context, with certain properties and control an orthogonal behavior. import requests def get(session, url): &quot;&quot;&quot;Utility method to HTTP GET a URL&quot;&quot;&quot; response = session.get(url, timeout=None) return response def post(session, url, data): &quot;&quot;&quot;Utility method to HTTP POST a URL with data parameters&quot;&quot;&quot; response = session.post(url, data=data, timeout=None) if response.status_code != requests.codes.ok: response.raise_for_status() return response with requests.Session() as s: # x-test header will be sent with every request s.headers.update({&#39;x-test&#39;: &#39;true&#39;}) data = {&#39;user&#39;: user, &#39;password&#39;: password, &#39;rememberne&#39;: &#39;false&#39;} r = post(s, &#39;https://portal.login.url&#39;, data) r = get(s, &#39;https://portal.home.url&#39;) Besides the session object, that gives you ability to add headers to all requests, you won’t have to worry about redirects. The library by default takes care of that with a default limit of up to 30. Happy hacking!" />
<meta property="og:description" content="I had to write a Python script some months ago, that would retrieve Solar energy data from a web site. It was basically a handful of HTTP calls, parse the response that was mainly in JSON, and store the results as JSON and CSV for processing later. Since it was such a small task, I used the Requests module instead of a complete web scraper. The HTTP requests had to be made with a time out, and also pass certain headers. I started customizing each call, until I learned about the Requests Session Objects. You create a session, as in ORM/JPA, where you can define a context, with certain properties and control an orthogonal behavior. import requests def get(session, url): &quot;&quot;&quot;Utility method to HTTP GET a URL&quot;&quot;&quot; response = session.get(url, timeout=None) return response def post(session, url, data): &quot;&quot;&quot;Utility method to HTTP POST a URL with data parameters&quot;&quot;&quot; response = session.post(url, data=data, timeout=None) if response.status_code != requests.codes.ok: response.raise_for_status() return response with requests.Session() as s: # x-test header will be sent with every request s.headers.update({&#39;x-test&#39;: &#39;true&#39;}) data = {&#39;user&#39;: user, &#39;password&#39;: password, &#39;rememberne&#39;: &#39;false&#39;} r = post(s, &#39;https://portal.login.url&#39;, data) r = get(s, &#39;https://portal.home.url&#39;) Besides the session object, that gives you ability to add headers to all requests, you won’t have to worry about redirects. The library by default takes care of that with a default limit of up to 30. Happy hacking!" />
<link rel="canonical" href="https://kinoshita.eti.br/2016/11/25/using-requests-session-objects-for-web-scraping.html" />
<meta property="og:url" content="https://kinoshita.eti.br/2016/11/25/using-requests-session-objects-for-web-scraping.html" />
<meta property="og:site_name" content="Bruno P. Kinoshita" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2016-11-25T00:00:00+13:00" />
<script type="application/ld+json">
{"description":"I had to write a Python script some months ago, that would retrieve Solar energy data from a web site. It was basically a handful of HTTP calls, parse the response that was mainly in JSON, and store the results as JSON and CSV for processing later. Since it was such a small task, I used the Requests module instead of a complete web scraper. The HTTP requests had to be made with a time out, and also pass certain headers. I started customizing each call, until I learned about the Requests Session Objects. You create a session, as in ORM/JPA, where you can define a context, with certain properties and control an orthogonal behavior. import requests def get(session, url): &quot;&quot;&quot;Utility method to HTTP GET a URL&quot;&quot;&quot; response = session.get(url, timeout=None) return response def post(session, url, data): &quot;&quot;&quot;Utility method to HTTP POST a URL with data parameters&quot;&quot;&quot; response = session.post(url, data=data, timeout=None) if response.status_code != requests.codes.ok: response.raise_for_status() return response with requests.Session() as s: # x-test header will be sent with every request s.headers.update({&#39;x-test&#39;: &#39;true&#39;}) data = {&#39;user&#39;: user, &#39;password&#39;: password, &#39;rememberne&#39;: &#39;false&#39;} r = post(s, &#39;https://portal.login.url&#39;, data) r = get(s, &#39;https://portal.home.url&#39;) Besides the session object, that gives you ability to add headers to all requests, you won’t have to worry about redirects. The library by default takes care of that with a default limit of up to 30. Happy hacking!","@type":"BlogPosting","url":"https://kinoshita.eti.br/2016/11/25/using-requests-session-objects-for-web-scraping.html","headline":"Using Requests Session Objects for web scraping","dateModified":"2016-11-25T00:00:00+13:00","datePublished":"2016-11-25T00:00:00+13:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://kinoshita.eti.br/2016/11/25/using-requests-session-objects-for-web-scraping.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/main.css"><link type="application/atom+xml" rel="alternate" href="https://kinoshita.eti.br/feed.xml" title="Bruno P. Kinoshita" /><link href="https://unpkg.com/pattern.css" rel="stylesheet">

</head>
<body>
<div id="mobile-menu"><div class="menu">
  <ul class="menu">
    <li class="item">
      <h1 id="kinow">KINOW</h1>
    </li>
    <li class="item">
      <a href="/" class="">About</a>
    </li>
    <li class="item">
      <a href="/blog/" class="current">Blog</a>
    </li>
    <li class="item">
      <a href="/portfolio/" class="">Portfolio</a>
    </li>
  </ul>
</div>
</div>
<div id="wrapper">
  <div id="sidebar"><div class="menu">
  <ul class="menu">
    <li class="item">
      <h1 id="kinow">KINOW</h1>
    </li>
    <li class="item">
      <a href="/" class="">About</a>
    </li>
    <li class="item">
      <a href="/blog/" class="current">Blog</a>
    </li>
    <li class="item">
      <a href="/portfolio/" class="">Portfolio</a>
    </li>
  </ul>
</div>
</div>
  <div id="content">
    <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Using Requests Session Objects for web scraping</h1>
    <span class="post-meta">
      <time class="dt-published" datetime="2016-11-25T00:00:00+13:00" itemprop="datePublished">Nov 25, 2016
      </time>
      <span>&mdash; star date: -306019.13.</span>
      <span>
        Number of words: 359.
      </span></span>
  </header>

  


  <div class="post-content e-content" itemprop="articleBody">
    <p>I had to write a Python script some months ago, that would retrieve Solar energy
data from a web site. It was basically a handful of HTTP calls, parse the response
that was mainly in JSON, and store the results as JSON and CSV for processing later.</p>

<p>Since it was such a small task, I used the <a href="http://docs.python-requests.org">Requests</a>
module instead of a complete web scraper. The HTTP requests had to be made with a
time out, and also pass certain headers. I started customizing each call, until
I learned about the Requests <a href="http://docs.python-requests.org/en/master/user/advanced/">Session Objects</a>.</p>

<p>You create a session, as in ORM/JPA, where you can define a context, with certain properties
and control an orthogonal behavior.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">requests</span>

<span class="k">def</span> <span class="nf">get</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">url</span><span class="p">):</span>
    <span class="s">"""Utility method to HTTP GET a URL"""</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span>

<span class="k">def</span> <span class="nf">post</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">url</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="s">"""Utility method to HTTP POST a URL with data parameters"""</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">!=</span> <span class="n">requests</span><span class="o">.</span><span class="n">codes</span><span class="o">.</span><span class="n">ok</span><span class="p">:</span>
        <span class="n">response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">response</span>

<span class="k">with</span> <span class="n">requests</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">s</span><span class="p">:</span>
    <span class="c1"># x-test header will be sent with every request
</span>    <span class="n">s</span><span class="o">.</span><span class="n">headers</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s">'x-test'</span><span class="p">:</span> <span class="s">'true'</span><span class="p">})</span>

    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s">'user'</span><span class="p">:</span> <span class="n">user</span><span class="p">,</span> <span class="s">'password'</span><span class="p">:</span> <span class="n">password</span><span class="p">,</span> <span class="s">'rememberne'</span><span class="p">:</span> <span class="s">'false'</span><span class="p">}</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">post</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="s">'https://portal.login.url'</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

    <span class="n">r</span> <span class="o">=</span> <span class="n">get</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="s">'https://portal.home.url'</span><span class="p">)</span>
</code></pre></div></div>

<p>Besides the session object, that gives you ability to add headers to all requests,
you won’t have to worry about redirects. The library by default takes care
<a href="https://github.com/kennethreitz/requests/blob/dfad00a6e84bc75b12468ca29ccf4f971c813fc8/requests/sessions.py#L110">of that</a>
with a <a href="https://github.com/kennethreitz/requests/blob/dfad00a6e84bc75b12468ca29ccf4f971c813fc8/requests/models.py#L54">default limit of up to 30</a>.</p>

<p>Happy hacking!</p>

  </div>

  <div class="tags">
    <p>
      Published Nov 25, 2016 in &ldquo;blog&rdquo; and tagged&nbsp;<a class="label" href="/tags#python">python</a>. Number of words: 359.
    </p>
  </div>

  <a class="u-url" href="/2016/11/25/using-requests-session-objects-for-web-scraping.html" hidden></a>
</article>

  </div>
</div>
</body>
</html>
