<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:url" content="https://kinoshita.eti.br/2012/09/20/running-word-count-example-on-a-hadoop-commodity-hardware-cluster-and-on-a-hadoop-local-installation.html"><meta property="og:site_name" content="kinow"><meta property="og:title" content="Running word-count example on a Hadoop commodity-hardware cluster and on a Hadoop local installation"><meta property="og:description" content="Last weekend I spent some hours assembling old computer parts to create my commodity hardware cluster for running Hadoop. I already had a local installation in my notebook, so I thought it would be cool to run the word-count example in both scenarios to see what would be the results.
But first, let's review the hardware configurations:
"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2012-09-20T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-29T16:14:50+02:00"><meta property="article:tag" content="Big Data"><meta property="og:image" content="https://kinoshita.eti.br/assets/photos/about/2023-me-closeup.png"><meta property="fb:admins" content="bruno.kinoshita"><meta name=twitter:title content="Running word-count example on a Hadoop commodity-hardware cluster and on a Hadoop local installation"><meta name=twitter:description content="Last weekend I spent some hours assembling old computer parts to create my commodity hardware cluster for running Hadoop. I already had a local installation in my notebook, so I thought it would be cool to run the word-count example in both scenarios to see what would be the results.
But first, let's review the hardware configurations:"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://kinoshita.eti.br/assets/photos/about/2023-me-closeup.png"><meta name=twitter:site content="@kinow"><link rel=alternate type=application/rss+xml href=https://kinoshita.eti.br/2012/09/20/running-word-count-example-on-a-hadoop-commodity-hardware-cluster-and-on-a-hadoop-local-installation/feed.xml title=kinow><link rel=stylesheet type=text/css href=https://kinoshita.eti.br/sass/main.c3626db3678f53e4a53e3e75c7b2fe34bb46566f00eb84692501404d0064c32a.css integrity="sha256-w2Jts2ePU+SlPj51x7L+NLtGVm8A64RpJQFATQBkwyo="><title>kinow | Running word-count example on a Hadoop commodity-hardware cluster and on a Hadoop local installation</title><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"posts","name":"Running word-count example on a Hadoop commodity-hardware cluster and on a Hadoop local installation","headline":"Running word-count example on a Hadoop commodity-hardware cluster and on a Hadoop local installation","alternativeHeadline":"","description":"\u003cp\u003eLast weekend I spent some hours assembling old computer parts to create my commodity hardware cluster for running Hadoop. I already had a local installation in my notebook, so I thought it would be cool to run the word-count example in both scenarios to see what would be the results.\u003c\/p\u003e\n\u003cp\u003eBut first, let\u0027s review the hardware configurations:\u003c\/p\u003e","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/kinoshita.eti.br\/2012\/09\/20\/running-word-count-example-on-a-hadoop-commodity-hardware-cluster-and-on-a-hadoop-local-installation.html"},"author":{"@type":"Person","name":"Bruno P. Kinoshita"},"creator":{"@type":"Person","name":"Bruno P. Kinoshita"},"accountablePerson":{"@type":"Person","name":"Bruno P. Kinoshita"},"copyrightHolder":"kinow","copyrightYear":"2012","dateCreated":"2012-09-20T00:00:00.00Z","datePublished":"2012-09-20T00:00:00.00Z","dateModified":"2024-04-29T16:14:50.00Z","publisher":{"@type":"Organization","name":"kinow","url":"https://kinoshita.eti.br/","logo":{"@type":"ImageObject","url":"https:\/\/kinoshita.eti.br\/","width":"32","height":"32"}},"image":"https://kinoshita.eti.br/","url":"https:\/\/kinoshita.eti.br\/2012\/09\/20\/running-word-count-example-on-a-hadoop-commodity-hardware-cluster-and-on-a-hadoop-local-installation.html","wordCount":"493","genre":["big data"],"keywords":[]}</script></head><body><header role=banner class=content><nav id=nav aria-label=Main role=navigation class=content><ul><li><a href=/><i data-feather=about></i> About</a></li><li><a href=/blog/><i data-feather=blog></i> Blog</a></li><li><a href=/portfolio/><i data-feather=portfolio></i> Portfolio</a></li></ul></nav></header><main><article class=content><h1>Running word-count example on a Hadoop commodity-hardware cluster and on a Hadoop local installation</h1><div class=metadata><i data-feather=calendar></i>
<time datetime=2012-09-20>Sep 20, 2012</time></div><aside><nav id=TableOfContents></nav></aside><p>Last weekend I spent some hours assembling old computer parts to create my commodity hardware cluster for running Hadoop. I already had a local installation in my notebook, so I thought it would be cool to run the word-count example in both scenarios to see what would be the results.</p><p>But first, let's review the hardware configurations:</p><hr><h2>Cluster set up</h2><h4>devcluster01 (NameNode)</h4><ul><li>Intel Core 2 Duo 2.3 GHz</li><li>4 GB</li><li>200 GB</li><li>100 Mbps Full Duplex network card</li></ul><h4>devcluster02</h4><ul><li>AMD Athlon 1.6 GHz</li><li>500 MB</li><li>4 GB</li><li>100 Mbps Full Duplex network card</li></ul><h4>devcluster03</h4><ul><li>Celeron 1.3 GHz</li><li>300MB</li><li>10 GB</li><li>100 Mbps Full Duplex network card</li></ul><h4>devcluster04 (TaskTracker)</h4><ul><li>Celeron 2.26 GHz</li><li>512 MB</li><li>80 GB</li><li>100 Mbps Full Duplex network card</li></ul><h4>devcluster05</h4><ul><li>AMD Duron 1.1 GHz</li><li>512MB</li><li>40 GB</li><li>100 Mbps Full Duplex network card</li></ul><h4>Network</h4><ul><li>Ethernet 10/100 D-Link hub</li></ul><hr><h2>Standalone installation</h2><ul><li>Intel Core i5 2.3 GHz (quad core)</li><li>6 GB</li><li>500 GB</li></ul><hr><p>Notice that there is one NameNode (exclusive) and one JobTracker (exclusive too). I'm following the default for a cluster installation, but will try switching a DataNode/TaskTracker with less computing power for the NameNode or the JobTracker (they were randomly selected), and using both servers as DataNode/TaskTracker too.</p><p>The word-count example that I used can be found at <a href=http://www.github.com/kinow/hadoop-wordcount title=http://www.github.com/kinow/hadoop-wordcount>https://github.com/kinow/hadoop-wordcount</a>. And the data used are free e-books from <a href=http://www.gutenberg.org/ title=Gutenberg>Gutenberg project</a>, saved as text plain UTF-8.</p><p>First I used the default data from this <a href=http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/#Copy_local_example_data_to_HDFS title="Hadoop Tutorial">tutorial</a>, that includes only three books. Then I increased to 8 books. Not happy with the result I tried 30, and finally 66 books. You can get the data from the same GitHub repository mentioned above.</p><p>Using the web interface I retrieved the total time to execute each job, and using the following R script, plotted the graph below (for more on plotting R graphs, check this <a href=http://www.harding.edu/fmccown/r/ title=Link>link</a>).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#000>a1</span> <span style=color:#000>=</span> <span style=color:#000>c</span>(<span style=color:#1c01ce>73</span>, <span style=color:#1c01ce>75</span>, <span style=color:#1c01ce>132</span>, <span style=color:#1c01ce>248</span>) <span style=color:#177500># time in the cluster</span>
</span></span><span style=display:flex><span><span style=color:#000>a2</span> <span style=color:#000>=</span> <span style=color:#000>c</span>(<span style=color:#1c01ce>40</span>, <span style=color:#1c01ce>48</span>, <span style=color:#1c01ce>121</span>, <span style=color:#1c01ce>224</span>) <span style=color:#177500># time running locally</span>
</span></span><span style=display:flex><span><span style=color:#000>files</span> <span style=color:#000>=</span> <span style=color:#000>c</span>(<span style=color:#1c01ce>3</span>, <span style=color:#1c01ce>8</span>, <span style=color:#1c01ce>30</span>, <span style=color:#1c01ce>66</span>) <span style=color:#177500># number of files used</span>
</span></span><span style=display:flex><span><span style=color:#000>plot</span>(<span style=color:#000>x</span><span style=color:#000>=</span><span style=color:#000>files</span>, <span style=color:#000>xlab</span><span style=color:#000>=</span><span style=color:#c41a16>&#34;Number of files&#34;</span>, <span style=color:#000>y</span><span style=color:#000>=</span><span style=color:#000>a1</span>, <span style=color:#000>ylab</span><span style=color:#000>=</span><span style=color:#c41a16>&#34;Time (s)&#34;</span>, <span style=color:#000>col</span><span style=color:#000>=</span><span style=color:#c41a16>&#34;red&#34;</span>, <span style=color:#000>type</span><span style=color:#000>=</span><span style=color:#c41a16>&#34;o&#34;</span>) <span style=color:#177500># plot cluster line</span>
</span></span><span style=display:flex><span><span style=color:#000>lines</span>(<span style=color:#000>y</span><span style=color:#000>=</span> <span style=color:#000>a2</span>, <span style=color:#000>x</span><span style=color:#000>=</span><span style=color:#000>files</span>, <span style=color:#000>type</span><span style=color:#000>=</span><span style=color:#c41a16>&#34;o&#34;</span>, <span style=color:#000>pch</span><span style=color:#000>=</span><span style=color:#1c01ce>22</span>, <span style=color:#000>lty</span><span style=color:#000>=</span><span style=color:#1c01ce>2</span>, <span style=color:#000>col</span><span style=color:#000>=</span><span style=color:#c41a16>&#34;blue&#34;</span>) <span style=color:#177500># add the local line</span>
</span></span><span style=display:flex><span><span style=color:#000>title</span>(<span style=color:#000>main</span><span style=color:#000>=</span><span style=color:#c41a16>&#34;Hadoop Execution in seconds&#34;</span>, <span style=color:#000>col.name</span><span style=color:#000>=</span><span style=color:#c41a16>&#34;black&#34;</span>, <span style=color:#000>font.main</span><span style=color:#000>=</span><span style=color:#1c01ce>2</span>)
</span></span><span style=display:flex><span><span style=color:#000>g_range</span> <span style=color:#000>&lt;</span> <span style=color:#000>-</span> <span style=color:#000>range</span>(<span style=color:#1c01ce>0</span>, <span style=color:#000>a1</span>, <span style=color:#000>files</span>)
</span></span><span style=display:flex><span><span style=color:#000>legend</span>(<span style=color:#1c01ce>2</span>, <span style=color:#000>g_range[2]</span>, <span style=color:#000>c</span>(<span style=color:#c41a16>&#34;Cluster&#34;</span>,<span style=color:#c41a16>&#34;Local&#34;</span>), <span style=color:#000>cex</span><span style=color:#000>=</span><span style=color:#1c01ce>0.8</span>, <span style=color:#000>col</span><span style=color:#000>=</span><span style=color:#000>c</span>(<span style=color:#c41a16>&#34;red&#34;</span>,<span style=color:#c41a16>&#34;blue&#34;</span>), <span style=color:#000>pch</span><span style=color:#000>=</span><span style=color:#1c01ce>21</span><span style=color:#000>:</span><span style=color:#1c01ce>22</span>, <span style=color:#000>lty</span><span style=color:#000>=</span><span style=color:#1c01ce>1</span><span style=color:#000>:</span><span style=color:#1c01ce>2</span>) <span style=color:#177500>#legend</span>
</span></span></code></pre></div><p style=text-align:center><a href=http://www.kinoshita.eti.br/wp-content/uploads/2012/09/Rplot.png><img src=http://www.kinoshita.eti.br/wp-content/uploads/2012/09/Rplot.png alt title=Graph width=550 height=400 class="aligncenter size-full wp-image-1019"></a></p><p><strong>The cluster is running slower than the standalone installation</strong>. During this week I'll investigate how to get better results with the cluster. I have three computers running tasks in the distributed cluster (the other two are the NameNode and JobTracker), and my notebook has four cores. It may be influencing the results. There is also the network latency, low memory in some nodes and changing the NameNode and JobTracker.</p><p>All in all, it's been fun to configure the cluster and run the experiments. It is good for practicing with Hadoop and HDFS, as well as getting a better idea on how to manage a cluster.</p></code><hr><p><em>Edit: JobTracker and TaskTracker were mixed up (thanks rretzbach)</em></p><p>Categories:
<a href=/categories/blog.html>Blog</a></p><p>Tags:
<a href=/tags/big-data.html>Big Data</a></p><aside class=links><a class=previous href=https://kinoshita.eti.br/2012/09/14/integrating-nutch-2.x-mysql-and-solr.html>&#171; Previous</a><a class=next href=https://kinoshita.eti.br/2012/09/25/paper-patterns-for-introducing-a-superclass-for-test-classes.html>Next &#187;</a></aside></article></main><footer role=contentinfo><ul id=social-media-icons><li><a href=https://www.instagram.com/brunokinoshita/><img src=/assets/icons/instagram.png alt="Instagram link"></a></li><li><a href=https://twitter.com/kinow/><img src=/assets/icons/twitter.png alt="Twitter link"></a></li><li><a href=https://github.com/kinow/><img src=/assets/icons/github.png alt="GitHub link"></a></li><li><a href=/feed.xml><img src=/assets/icons/news.png alt="RSS feed link"></a></li></ul></footer></body></html>